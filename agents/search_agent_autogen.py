import asyncio
from querying.query import Retrieval
from typing import Literal
from autogen_core.models import UserMessage, ModelInfo
from autogen_ext.models.ollama import OllamaChatCompletionClient
from autogen_core.tools import FunctionTool
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from pydantic import BaseModel
import time

retrieval_component = Retrieval()


async def search_tool(query: str) -> str:
    global retrieval_component
    answer = retrieval_component.no_context(query)
    return answer

async def main():
    # Also can config with a json file
    aina_model = OllamaChatCompletionClient(
        # llama3.1:8b   
        #model="hf.co/stepii/salamandra-7b-instruct-tools-GGUF:Q8_0", 
        model="llama3.1:8b",
        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family="unknown", structured_output=True),
        #model_client_stream=True,
    )

    agent = AssistantAgent(
        name="assistant", 
        model_client=aina_model, 
        tools=[search_tool],
        #max_tool_iterations=5,
        reflect_on_tool_use=True,
        system_message="You are a system capable to search into a vectordb and give a response based on the text generated by the model connected.",
    )
    
    tasks = ["What is a vectordb?", "How does a vector database differ from a traditional relational database?", "What are the typical use cases of a vector database?"]
    for question in tasks:
        await Console(agent.run_stream(task=question))

    await aina_model.close()

if __name__ == "__main__":
    start = time.time()
    asyncio.run(main())
    print(f"Elapsed time: {time.time() - start}")